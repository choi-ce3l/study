{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': \"[단독] '침출수 줄줄' 여수산단 대체녹지, 오염된 토사로 조성됐다\", 'body': '여수산단 대체녹지에서 1년 넘게 발암물질이 섞인 침출수가 흘러나오고 있다는 소식, 얼마 전 전해드렸는데요. 그런데 대체녹지 조성 당시, 오염된 토사가 사용됐을 가능성이 큰 것으로 확인됐습니다. 대체녹지 조성 당시 토사의 오염여부를 확인해야 이 같은 문제를 방지할 수 있었지만, 법적 기준에 미치지 못 해 별다른 관리가 이뤄지지 않았습니다. 이 글자크기로 변경됩니다. (예시) 가장 빠른 뉴스가 있고 다양한 정보, 쌍방향 소통이 숨쉬는 다음뉴스를 만나보세요. 다음뉴스는 국내외 주요이슈와 실시간 속보, 문화생활 및 다양한 분야의 뉴스를 입체적으로 전달하고 있습니다. 【 앵커멘트 】여수산단 대체녹지에서 1년 넘게 발암물질이 섞인 침출수가 흘러나오고 있다는 소식, 얼마 전 전해드렸는데요. 토양오염의 원인을 찾지 못했다며 녹지를 조성한 기업도, 감독해야 할 행정기관도 손을 놓고 있습니다. 그런데 대체녹지 조성 당시, 오염된 토사가 사용됐을 가능성이 큰 것으로 확인됐습니다. 박성호 기자의 보도입니다. 【 기자 】여수산단 대체녹지에서 오염된 침출수가 처음 확인된 것은 지난해 7월. 1년 넘게 문제가 계속되고 있지만 녹지를 조성한 기업들도, 기부채납 받은 여수시도 책임을 미룬 채 지켜만 보고 있습니다. 이런 와중에 대체 녹지를 조성한 토사 가운데 절반 이상을, 건축폐기물 4천8백톤이 불법매립됐던 곳에서 가져온 사실이 새롭게 드러났습니다. KBC가 단독 입수한 대체녹지 1구역 조성 토사반입 내역 서류입니다. 전체 토사 5만㎥ 중 3만㎥, 전체 57%의 토사가 한 곳에서 반입됐는데, 이 곳은 건축폐기물 불법매립이 뒤늦게 드러난 곳이었습니다. 전문가들은 건축폐기물 불법매립으로 오염된 토사가 유입됐을 경우 침출수 등 문제가 발생할 수 있다고 말합니다. ▶ 싱크 : 환경공학 전문가(음성변조)- \"폐기물에 의해서 그 토사에 그게(오염원) 농축이 되고 그 농축된 토사에 의해서 침출이 되는 거, 그거는 이제 저희가 많이 확인이 돼왔죠.\" 대체녹지 조성 당시 토사의 오염여부를 확인해야 이 같은 문제를 방지할 수 있었지만, 법적 기준에 미치지 못 해 별다른 관리가 이뤄지지 않았습니다. ▶ 인터뷰 : 박민수 / 여수시 공원과장- \"저희가 공원 녹지를 조성할 때 10만 평방미터 이상이 되면 사전에 토양 조사를 해야 되는데 여기는 6만 2천 평방미터 정도 돼서 그 토양 조사나 이런 것들은 생략하고 저희가 공원을 조성했습니다.\" 토양오염은 드러났지만 그 원인을 찾지 못했다는 이유로 1년 넘게 방치됐던 여수산단 대체녹지 문제가 해결의 실마리를 찾을 수 있을지 주목됩니다. KBC 박성호입니다. #사건사고 #전남 #여수산단 #침출수 Copyright © kbc광주방송. 무단전재 및 재배포 금지. 이 뉴스에 대해 의견을 나눠보세요.  톡방 종료까지 00:00:00 남았습니다.', 'reaction': {'reaction': 'OPTION_B', 'value': 0}}\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import json \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#동기웹페이지 bs로 불러오기 -> head, body가 동기 페이지임.\n",
    "response=requests.get('https://v.daum.net/v/20240811221440047')\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "#head, body에서 내용만 갖고 오기\n",
    "head_tag=soup.select_one('h3').text\n",
    "body_tags=soup.find_all('p')\n",
    "body_tag = ' '.join([tag.text.strip() for tag in body_tags])\n",
    "\n",
    "#비동기웹 json으로 갖고오기, header 설정해야 됨.\n",
    "res2=requests.get('https://action.daum.net/apis/v1/reactions/home?itemKey=20240811221440047',headers={'authorization':'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiI4Mjk4ZmE1ZC1hZDFlLTRmZDctODE2NS04OTY0NWMzYTM1ODkiLCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgiLCJmb3J1bV9rZXkiOiJuZXdzIiwiZm9ydW1faWQiOi05OSwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJhdXRob3JpdGllcyI6WyJST0xFX0NMSUVOVCJdLCJzY29wZSI6W10sImV4cCI6MTcyMzUzMjU4Mn0.4rdFTnynsR_YQaxmZsucx3-K4tTo_C12WYpt6vj_UK4'})\n",
    "json1=res2.json()\n",
    "\n",
    "#최종 값들 저장\n",
    "result=[]\n",
    "values=list(json1['item']['stats'].values())\n",
    "reaction={}\n",
    "\n",
    "#dict에 넣어주기\n",
    "for i in values:\n",
    "    for react in json1['item']['stats']:\n",
    "        reaction=dict(reaction=react,value=i)\n",
    "\n",
    "result=dict(title=head_tag,body=body_tag,reaction=reaction)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#동기웹페이지 bs로 불러오기 -> head, body가 동기 페이지임.\n",
    "response=requests.get('https://v.daum.net/v/20240811221440047')\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "#head, body에서 내용만 갖고 오기\n",
    "head_tag=soup.select_one('h3').text\n",
    "body_tags=soup.find_all('p')\n",
    "body_tag = ' '.join([tag.text.strip() for tag in body_tags])\n",
    "\n",
    "#비동기웹 json으로 갖고오기, header 설정해야 됨.\n",
    "res2=requests.get('https://action.daum.net/apis/v1/reactions/home?itemKey=20240811221440047',headers={'authorization':'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiI4Mjk4ZmE1ZC1hZDFlLTRmZDctODE2NS04OTY0NWMzYTM1ODkiLCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgiLCJmb3J1bV9rZXkiOiJuZXdzIiwiZm9ydW1faWQiOi05OSwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJhdXRob3JpdGllcyI6WyJST0xFX0NMSUVOVCJdLCJzY29wZSI6W10sImV4cCI6MTcyMzUzMjU4Mn0.4rdFTnynsR_YQaxmZsucx3-K4tTo_C12WYpt6vj_UK4'})\n",
    "\n",
    "#최종 값들 저장\n",
    "result=dict(title=head_tag,body=body_tag,reactions={'RECOMMEND':0,'LIKE':0,'IMPRESS':0,'ANGRY':0,'SAD':0})\n",
    "reactions=json1['item']['stats']\n",
    "print(reactions)\n",
    "\n",
    "for react in result[0]['reactions']:\n",
    "    result[0]['reactions'][react]=reactions[react]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import json \n",
    "\n",
    "result = []\n",
    "\n",
    "response = requests.get('https://v.daum.net/v/20240811221440047')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "title = soup.select_one('h3.tit_view').text.strip()\n",
    "body = soup.select('div.article_view section p')\n",
    "body = ' '.join([p_tag.text.strip() for p_tag in body])\n",
    "print(body)\n",
    "\n",
    "result.append(dict(\n",
    "    title = title,\n",
    "    body = body,\n",
    "    reactions = {\n",
    "        'RECOMMEND': 0,\n",
    "        'LIKE': 0,\n",
    "        'IMPRESS': 0,\n",
    "        'ANGRY': 0,\n",
    "        'SAD': 0\n",
    "    }\n",
    "))\n",
    "\n",
    "\n",
    "# 리액션 \n",
    "headers = {\n",
    "'Authorization':'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiJmNWY5NWYzYi03OTkyLTQzNTEtODg5My05Yzc0ZWEyOGQwMzYiLCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgiLCJmb3J1bV9rZXkiOiJuZXdzIiwiZm9ydW1faWQiOi05OSwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJhdXRob3JpdGllcyI6WyJST0xFX0NMSUVOVCJdLCJzY29wZSI6W10sImV4cCI6MTcyMzUzMzM4OX0.Rq75EWMFiNnFSRMPPzjx9_JjxPhesoNqRWGJBbhEQCM'\n",
    "}\n",
    "\n",
    "react_res = requests.get('https://action.daum.net/apis/v1/reactions/home?itemKey=20240811221440047', headers=headers)\n",
    "\n",
    "reactions = react_res.json()['item']['stats']\n",
    "print(reactions)\n",
    "\n",
    "for react in result[0]['reactions']:\n",
    "    print(react)\n",
    "    result[0]['reactions'][react] =  reactions[react]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re # 정규표현식 사용위한 모듈 \n",
    "\n",
    "\n",
    "allNewsDatas = []\n",
    "num = 1\n",
    "while True:\n",
    "\n",
    "    res = requests.get(f'https://news.daum.net/breakingnews/society/affair?page={num}&regDate=20240811')\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "\n",
    "    # 페이지 태그 찾기 \n",
    "    page_text = soup.select_one('span.inner_paging em').text.strip()\n",
    "    page_number = re.findall('[0-9]+', page_text)[0]  # 텍스트에서 모든 숫자열을 찾아 리스트로 반환 \n",
    "\n",
    "\n",
    "    print('현재 페이지: ', page_number) # num과 실제페이지가 다르면 끝내기 \n",
    "    if str(num) != page_number:\n",
    "        print('크롤링 완료')\n",
    "        break \n",
    "\n",
    "\n",
    "    # 해당 페이지의 기사 링크 크롤링 \n",
    "    a_tags = soup.select('ul.list_news2.list_allnews li strong.tit_thumb a.link_txt')\n",
    "    for a in a_tags:\n",
    "        link = a.attrs['href']\n",
    "\n",
    "\n",
    "        news_res = requests.get(link)\n",
    "        news_soup = BeautifulSoup(news_res.text, 'html.parser')\n",
    "\n",
    "\n",
    "        # 제목 \n",
    "        title = news_soup.select_one('h3.tit_view').text.strip()\n",
    "        # print('title: ', title)\n",
    "        \n",
    "\n",
    "        # 본문 \n",
    "        if news_soup.select_one('div.article_view section div'): # div에 내용이 존재하면 포함시기키 \n",
    "\n",
    "\n",
    "            body = list(news_soup.select_one('div.article_view section').strings)\n",
    "            body = ' '.join(line.strip() for line in body).strip()\n",
    "            # print(body)\n",
    "\n",
    "        else: # div태그가 없는 경우 (div태그 내에 내용이 없는 경우)\n",
    "            p_tags = news_soup.select('div.article_view section p')\n",
    "            body  = ' '.join([p_tag.text.strip() for p_tag in p_tags]) # 해시태그 제외하기 \n",
    "            # print(body)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 데이터 담을 딕셔너리 생성 \n",
    "        news = dict(\n",
    "            page = page_number, \n",
    "            title = title,\n",
    "            body = body,\n",
    "            reactions = {\n",
    "                'RECOMMEND':0,\n",
    "                'LIKE':0,\n",
    "                'IMPRESS':0,\n",
    "                'ANGRY':0,\n",
    "                'SAD':0\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        # 리엑션 \n",
    "        headers = {\n",
    "            'authorization':'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiJmNWY5NWYzYi03OTkyLTQzNTEtODg5My05Yzc0ZWEyOGQwMzYiLCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgiLCJmb3J1bV9rZXkiOiJuZXdzIiwiZm9ydW1faWQiOi05OSwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJhdXRob3JpdGllcyI6WyJST0xFX0NMSUVOVCJdLCJzY29wZSI6W10sImV4cCI6MTcyMzUzMzM4OX0.Rq75EWMFiNnFSRMPPzjx9_JjxPhesoNqRWGJBbhEQCM'\n",
    "        }\n",
    "\n",
    "\n",
    "        itmeKey = link.split('/')[-1] # 1페이지의 링크에서 itemKey 갖고 오기 \n",
    "        react_res = requests.get(f'https://action.daum.net/apis/v1/reactions/home?itemKey={itmeKey}', headers=headers)\n",
    "        reactions = react_res.json()['item']['stats']\n",
    "        # print(reactions)\n",
    "        # print()\n",
    "\n",
    "        for i, react in enumerate(reactions):\n",
    "            if react in news['reactions'] :\n",
    "                news['reactions'][react] = reactions[react]\n",
    "        \n",
    "        allNewsDatas.append(news)\n",
    "\n",
    "        # print(news)\n",
    "        break\n",
    "\n",
    "    num += 1 \n",
    "    \n",
    "    break\n",
    "print(len(allNewsDatas))\n",
    "\n",
    "        \n",
    "# json파일로 저장 \n",
    "with open('allPagesDaumNews.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(allNewsDatas, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 숙제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임시현, ‘3관왕’ 달성···남수현 은메달까지, 한국 女 개인전도 쓸었다\n",
      "임시현(21)이 2024 파리올림픽에서 3관왕에 올랐다. 임시현은 3일 프랑스 파리 앵발리드에서 열린 2024 파리올림픽 양궁 여자 개인전 결승에서 남수현(19)과 대결, 세트점수 7-3으로 승리했다. 앞서 여자 단체전 금메달에 이어 2일 김우진과 함께 혼성 단체전 금메달을 딴 임시현은 여자 개인전까지 우승하면서 3개의 금메달을 쓸어담았다. 혼성 단체전이 도입된 2020 도쿄올림픽에서 안산에 이어 이번 대회에서도 한국 선수 임시현이 3관왕을 차지했다. 더불어 한국 여자 양궁은 2012년 런던올림픽(기보배)부터 2016년 리우데자네이루 올림픽(장혜진), 2021년 도쿄올림픽(안산)에 이어 2024년 파리올림픽의 임시현이 우승하면서 개인전 4연패를 달성했다. 임시현은 4강과 결승전에서 모두 한국 선수와 대결했다. 한국 선수 3명이 나란히 4강에 진출하면서 대진상 임시현이 전훈영과 4강에서 만났고 세트 점수 6-4로 승리하고 결승에 나갔다. 프랑스 선수를 꺾고 결승에 올라온 남수현과 치른 결승전에서는 접전 끝에 세트 점수 7-3으로 이겼다. 1세트에서 둘이 나란히 10점 두 발과 9점을 쏴 세트 점수 1점씩 나눠가졌다. 임시현은 2세트에도 9점 뒤 10점 두 발을 쐈고 남수현이 두번째 슛에서 7점을 맞히면서 임시현이 앞서 갔다. 임시현은 3세트에서는 3발을 모두 10점으로 맞혀 따냈고, 4세트에서는 10점-9점-10점을 쐈으나 이번에는 남수현이 10점 3발을 쏴 세트 점수 2점을 가져갔다. 5-3으로 임시현이 앞선 채 이어진 5세트에서 임시현이 첫 발에 10점을 쏘고 남수현이 8점을 쐈다. 두번째 화살에는 임시현이 8점, 남수현이 10점을 쐈다. 동률 상태에서 마지막 한 발이 남았고 임시현이 10점을 넣은 뒤 남수현이 8점을 넣으면서 임시현이 금메달, 남수현의 은메달이 확정됐다. 4강전에서 임시현에게 져 동메달 결정전으로 향한 전훈영은 프랑스 리사 바벨랭과 접전 끝에 세트 점수 4-6으로 져 메달 획득에는 실패했다. 한국은 이로써 여자 단체전, 남자 단체전, 혼성 단체전에 이어 여자 개인전까지 양궁에 5개 걸린 금메달 중 4개를 쓸어담았다. 4일에는 남자 개인전이 열린다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import json \n",
    "\n",
    "result = []\n",
    "\n",
    "response = requests.get('https://n.news.naver.com/mnews/article/032/0003312694')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "title = soup.select_one('#title_area span').text.strip()\n",
    "body=soup.select_one('#dic_area').find_all(string=True,recursive=False)\n",
    "body=' '.join(body).strip()\n",
    "print(title)\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
